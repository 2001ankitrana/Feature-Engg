{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1/ What is parameter?\n",
        "\n",
        "- Parameter is a variable that is used to pass information to a function,when defining a function we specify the parameters that it will accept, when you call the function we provide values for those parameters which are called arguments."
      ],
      "metadata": {
        "id": "Mu57i7o8y9St"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2/ What is correlation,-ve corelation mean?\n",
        "\n",
        "- Correlation is a ststistical measure that describe the strength and direction of a linear relationship between two or more variables.\n",
        "\n",
        "-ve correlation: As one variable increases, the other tends to decrease, eg, hours of exercise and body fat percentage often have -ve correlation."
      ],
      "metadata": {
        "id": "Zk0HcQ_P1mc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/ Define ML and its components.\n",
        "\n",
        "- ML is a subfield if AI that focuses on enabling computer systems to learn from data without being explicitly programmed, it's about creating algorithms that allow computers to identify patterns, make predictions, and improve their performance over time based on the data they are exposed to.\n",
        "\n",
        "key components;\n",
        "\n",
        "Data, Representation, Evaluation, Optimization,"
      ],
      "metadata": {
        "id": "C8r0pE1j2ex3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4/ How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "- The loss value is a crucial metric in machine learning, indicating how well a model is performing, a lower loss value generally suggests better performance, but it's essential to consider factors like overfitting and validation loss to ensure the model generalizes well to new data.\n",
        "\n",
        "Interpreting loss values:\n",
        "\n",
        "high loss, low loss,\n",
        "\n",
        "considerations- overfitting and validation loss."
      ],
      "metadata": {
        "id": "ZFMrTkMe3tCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5/ What are continuous and categorical variables?\n",
        "\n",
        "- Continuous:These are numeric variables that can take on any value within a given range, they represent measurements and can be meaningfully divided into smaller increments. eg, height, temperature, weight.\n",
        "\n",
        "Categorical: These are variables that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit od observation to a particular group or nominal category on the basis of some qualitative property.\n",
        "eg. gender, eye color,\n",
        "\n"
      ],
      "metadata": {
        "id": "4RYK60c85JCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6/ How do we handle categorical variables in ML,common techniques?\n",
        "\n",
        "- Most ML algorithms work best with numerical data, categorical variables, which represent categories or groups, need to be converted into a numerical format before they can be used in these algorithms,\n",
        "\n",
        "some techniques to handle:\n",
        "\n",
        "- One hot encoding\n",
        "- Label encoding\n",
        "- Ordinal encoding"
      ],
      "metadata": {
        "id": "QiOPcxoe6nBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7/ What do you mean by training and testing a dataset?\n",
        "\n",
        "- Training Dataset: it is the portion of your data used to teach a ML model to recognize patterns and make predictions, the model learns from the relationship between features(input variables) and the target variable with in dataset\n",
        "\n",
        "Testing Dataset: The testing dataset is a seperate portion of your data that is held out from the training process, it's used to evaluate how well your trained model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "UkHEqJq075iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8/ What is sklearn.preprocessing?\n",
        "\n",
        "- The sklearn.preprocessing module provides a variety of functions and classes for transforming raw feature vectors into a representation that is more suitable for ML algorithms.\n"
      ],
      "metadata": {
        "id": "sUQh5I8d8Bml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9/ What is test set?\n",
        "\n",
        "- In ML,test set is aportion of your dataset that is held back and not used during the training process of your model, it's a completely unseen set of data that is used to evaluate the final performance of your trained model.\n",
        "\n",
        "why it is important-\n",
        "\n",
        "Unbiased evaluation, Avoiding overfitting, Model selection."
      ],
      "metadata": {
        "id": "uq5D309E8Kha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10/ How do we split data for model fitting in python, how do we approach ML problem?\n",
        "\n",
        "- Splitting data into training validation, and testing sets is crucial to avoid overfitting and obtain a realistic evaluation of your model's performance,\n",
        "\n",
        "by using-\n",
        "\n",
        ": train_test_split\n",
        ":random_state\n",
        ":Stratification\n",
        "\n",
        "General approach to ML problems:\n",
        "\n",
        "Problem definition\n",
        "\n",
        "Data collection and preperation\n",
        "\n",
        "Feature engineering\n",
        "\n",
        "Model selection\n",
        "\n",
        "Model training\n",
        "\n",
        "Model evaluation\n",
        "\n",
        "Hyperparameter tuning\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RodQciSW8PXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11/ Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "- EDA is the process of investigating a dataset to understand its main characteristics,patterns, and potential issues.\n",
        "\n",
        "performing EDA:\n",
        "\n",
        "1-Data loading and inspection\n",
        "\n",
        "2-Univariate Analysis\n",
        "\n",
        "3-Bivariate analysis\n",
        "\n",
        "4-Missing value handling\n",
        "\n",
        "5- Outlier detection"
      ],
      "metadata": {
        "id": "EXvjz5k18fPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14/ How can find correlation between variables in python?\n",
        "\n",
        "-  Using pandas corr(): this methos in pandas is a straightforward way to calculate the correlation between all numerical columns in a Dataframe.\n",
        "\n",
        "-using NumPy corrcoef(): it can be used to calculate correlation between two or more arrays.\n",
        "\n",
        "- using Seaborn heatmap(): can be used to visualize the correlation matrix as a heatmap,making it easier to identify patterns and relationships."
      ],
      "metadata": {
        "id": "wb5VxtrZ88He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15/ What is causation? difference between correlation and causation with a example.\n",
        "\n",
        "- Causation implies a cause-and-effect relationship between two variables, it means that a change in one variable directly cause a change in another variable, establishing causation requires more than just observing a correlation;it requires careful experimental design and analysis to rule out other potential explanations.\n",
        "\n",
        "Correlation is a ststistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "why is it important to distinguish between correlation and causation:\n",
        "\n",
        "Misinterpretations, Decision-making.\n"
      ],
      "metadata": {
        "id": "S6N9AcOG9H47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16/ What is Optimizer, different types of optimizers and eg?\n",
        "\n",
        "- In ML, an optimizer is an algorithm or method used to adjust the parameters of a model in order to minimize the loss function and improve its performance,it plays a crucial role in the training process,guiding the model towards the optimal set of parameters that best fit the training data.\n",
        "\n",
        "Types-\n",
        "\n",
        "Gradient Descent\n",
        "\n",
        "Stochastic Gradient Descent\n",
        "\n",
        "Mini-batch gradient descent\n",
        "\n",
        "Momentum\n",
        "\n",
        "Adagrad\n",
        "\n",
        "RMSprop"
      ],
      "metadata": {
        "id": "IamP0DiA9UoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17/ What is sklearn.linear_model?\n",
        "\n",
        "- This module provides a variety of classes for implementing linear models,which are a fundamental class of machine learning algorithms used for both regression and classification tasks.\n",
        "\n",
        "Common classes in this module:\n",
        "\n",
        "-Linear Regression\n",
        "-Logistic Regression\n",
        "-Ridge Regression\n",
        "-Lasso Regression\n",
        "-Elastic Net\n"
      ],
      "metadata": {
        "id": "dn42S5XM9fds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18/ What does model.fit()do? what arguments must be given?\n",
        "\n",
        "- In scikit-learn, model.fit() is a method used to train a machine learning model, it's the core step where the model learns patterns and relationships from the training data.\n",
        "\n",
        "what happens during model.fit()\n",
        "\n",
        "- Data Input\n",
        "-Parameter Estimation\n",
        "-Learning\n",
        "-Model is ready\n"
      ],
      "metadata": {
        "id": "1vVg7A8Y9nXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19/ What does model.predict()do? what arguments must e given.\n",
        "\n",
        "- Model.predict() is a method used to make predictions on new,unseen data after machine learning model has been trained using model.fit(),it's the step where you apply the learned patterns from the training data to make informed guesses about the target variable for new instances.\n",
        "\n",
        "how it works:\n",
        "\n",
        "- Input data\n",
        "- Applying learned patterns\n",
        "- Output\n",
        "\n"
      ],
      "metadata": {
        "id": "RCYutOAY9zbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20/ What is feature scaling? how does it help in ML.\n",
        "\n",
        "- It is a preprocessing technique used to transform numerical features in a dataset to a similar scale,this often done to prevent features with larger values from dominating the learning process and to ensure that all features contribute equally to the model's performance.\n",
        "\n",
        "why it is important in ML;\n",
        "\n",
        "- Algorithm performance\n",
        "- Faster Convergence\n",
        "-Improved Model Accuracy\n"
      ],
      "metadata": {
        "id": "ImiqrxQY-DZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21/ How do we perform scaling in Python?\n",
        "\n",
        "- 1 Import necessary libraries\n",
        "\n",
        "2 load ypur data\n",
        "\n",
        "3 select features to scale\n",
        "\n",
        "4 choose a scaling technique\n",
        "\n",
        "5 create a scaler object\n",
        "\n",
        "6 fit the scaler\n",
        "\n",
        "7 transform the data\n",
        "\n",
        "8 replace original features"
      ],
      "metadata": {
        "id": "hvn3I1ov-RGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22/ How do we split data for model fitting in python.\n",
        "\n",
        "- Before fitting a machine learning model, it's crucial to split your data into different sets:\n",
        "\n",
        " - training set\n",
        " -validation set\n",
        " -testing set\n",
        "\n",
        "Splitting data helps prevent overfitting, where the model performs well on training data but poorly on new data, and provides a more realistic estimate of the model's generalization ability.\n",
        "\n",
        "Using (train_test_split)"
      ],
      "metadata": {
        "id": "M2E0onDq-Y78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23/ Explain data encoding?\n",
        "\n",
        "- Data encoding is the process of converting categorical data into numerical data that machine learning algorithms can understand and work with, many machnine learning algorithms require numerical input,and categorical data needs to be transformed into a suitable numerical representation before being used in these algorithms.\n",
        "\n",
        "why it is important;\n",
        "\n",
        "Algorithm compatibility\n",
        "Improved performance"
      ],
      "metadata": {
        "id": "qEpjOK71-otc"
      }
    }
  ]
}